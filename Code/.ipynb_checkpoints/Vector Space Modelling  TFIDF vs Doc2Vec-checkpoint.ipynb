{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector Space Modelling for text : tf-idf and doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all modules\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tag_and_training_data(filename):\n",
    "    '''takes the input file and returns  tokenized sentences and document tags as separate lists'''\n",
    "    tags=[]\n",
    "    documents=[]\n",
    "    line_counter=1\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            #skip first line\n",
    "            if line_counter==1:\n",
    "                line_counter=line_counter+1\n",
    "                continue\n",
    "            #Initialize the token list for line\n",
    "            tags.append(int(line[:1]))\n",
    "            documents.append(line[2:])\n",
    "    return tags,documents\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y,X=get_tag_and_training_data('Data/trainingdata.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#75:25 training test split\n",
    "Y_train,Y_test=Y[:4120],Y[4120:]\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit_transform(X)\n",
    "freq_term_matrix = count_vectorizer.transform(X)\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)\n",
    "tf_idf_matrix = tfidf.transform(freq_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95457875457875463"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train logistic regression model\n",
    "X_train,X_test=tf_idf_matrix[:4120],tf_idf_matrix[4120:]\n",
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "logreg.fit(X_train,Y_train)\n",
    "pred=logreg.predict(X_test)\n",
    "accuracy_score(Y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2nd Category is BUSINESS NEWS,so lets test out a news peice on TESLA\n",
    "logreg.predict(tfidf.transform((count_vectorizer.transform([\"Tesla Inc. said on Friday it had raised about $1.2-billion (U.S.), roughly 20 per cent more than it had planned, by selling common shares and convertible debt, ahead of the launch of the crucial Model 3 sedan.\"]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73626373626373631"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initialize the Multinomial Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train,Y_train)\n",
    "nb_pred=clf.predict(X_test)\n",
    "accuracy_score(Y_test, nb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data already loaded as lists of sentences in X and Y\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(X):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "model = doc2vec.Doc2Vec(docs, size = 160, window = 10, min_count = 7, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#making training and test sets\n",
    "wb_Y_train,wb_Y_test=Y_train,Y_test\n",
    "wb_X=[]\n",
    "for i in range(len(X)):\n",
    "    wb_X.append(model.docvecs[i])\n",
    "wb_X_train=wb_X[:4120]\n",
    "wb_X_test=wb_X[4120:]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76190476190476186"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_logreg = linear_model.LogisticRegression(C=1e4)\n",
    "wb_logreg.fit(wb_X_train,wb_Y_train)\n",
    "wb_pred=wb_logreg.predict(wb_X_test)\n",
    "accuracy_score(wb_Y_test, wb_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58388278388278392"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_clf = GaussianNB()\n",
    "wb_clf.fit(wb_X_train,wb_Y_train)\n",
    "wb_nb_pred=wb_clf.predict(wb_X_test)\n",
    "accuracy_score(wb_Y_test, wb_nb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
